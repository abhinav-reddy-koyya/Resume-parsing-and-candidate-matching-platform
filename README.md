ğŸ“„ Resume Parsing & Candidate Matching Platform

ğŸ“˜ Summary

The Resume Parsing & Candidate Matching Platform is a Streamlit-powered recruitment assistant designed to simplify resume screening.
Recruiters can upload multiple resumes (PDF/DOCX), extract structured details (name, email, phone, skills, education, experience), and automatically compute a Predicted Match Score (%) against a job description.

The system provides:

A Candidate Dashboard to search and filter applicants

Analytics & Visualizations to see skills distribution and score histograms

SQLite database integration for persistence

CSV export for parsed and filtered candidate data

âœ¨ Key Features
ğŸ“¥ Upload Resumes

Upload multiple resumes in PDF/DOCX format

Automatic text extraction and parsing of candidate details:

Name, email, phone

Skills, education, experience

ğŸ“ Job Description Matching

Paste a Job Description â†’ system computes a Predicted Match Score (%) for each resume

Scores are saved in DB and exported in CSV

Score Colors:

ğŸŸ¢ Green â‰¥ 80 â†’ Strong fit

ğŸŸ  Orange 50â€“79 â†’ Medium fit

ğŸ”´ Red < 50 â†’ Weak fit

ğŸ“Š Candidate Dashboard

Search candidates by skills / name / email

Filter by minimum score or email availability

Export filtered candidate list as CSV

ğŸ“ˆ Analytics Dashboard

Bar chart of top skills across all candidates

Histogram of Predicted Match Score distribution

ğŸ’¾ Database Integration

Uses SQLite (resume_db.sqlite3) for persistence

Resumes stay saved between runs

Option to reset/clear database

ğŸ›  Tech Stack

Frontend/UI: Streamlit

Backend: Python 3 + SQLite

NLP: SpaCy (NER), Regex (email/phone)

Visualization: Matplotlib, Seaborn

Data Handling: Pandas

ğŸ“‚ Folder Structure
resume-parser/
â”‚â”€â”€ app.py                 # Main Streamlit app
â”‚â”€â”€ requirements.txt       # Dependencies
â”‚â”€â”€ README.md              # Documentation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ resumes/           # Uploaded resumes
â”‚   â””â”€â”€ parsed/            # Parsed CSVs
â”‚   â””â”€â”€ resume_db.sqlite3  # SQLite database
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ skills.pkl         # Predefined skill set (for extraction)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ parser.py          # PDF/DOCX text reader
â”‚   â”œâ”€â”€ extractor.py       # Extractor functions (skills, contact info, etc.)
â”‚   â”œâ”€â”€ job_matcher.py     # JD vs Resume scoring logic
â”‚   â””â”€â”€ db.py              # SQLite database helpers

âš™ï¸ Setup & Installation
1. Clone the repository
git clone https://github.com/your-username/resume-parser.git
cd resume-parser

2. Create a virtual environment
python -m venv venv


Activate it:

Windows: venv\Scripts\activate

Mac/Linux: source venv/bin/activate

3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

4. Download SpaCy model
python -m spacy download en_core_web_sm

5. Run the application
streamlit run app.py


Then open ğŸ‘‰ http://localhost:8501

ğŸš€ Usage Guide
ğŸ“¥ Upload & Parse Tab

Upload resumes (PDF/DOCX)

(Optional) Paste a Job Description to compute match scores

Parsed resumes appear in a table with Predicted Score %

Export as CSV

ğŸ“Š Candidate Dashboard Tab

Search candidates by skills, name, or email

Apply filters:

Minimum Predicted Score

Email required or not

Export filtered results as CSV

ğŸ“ˆ Analytics Tab

View Top Skills Across Candidates (bar chart)

Visualize Predicted Score Distribution (histogram)

ğŸ“Š Example Predicted Scores

90% (ğŸŸ¢ Green) â†’ Excellent fit

65% (ğŸŸ  Orange) â†’ Decent fit, may need training

40% (ğŸ”´ Red) â†’ Weak fit

ğŸ”® Future Improvements

Use TF-IDF / BERT embeddings for semantic JDâ€“resume matching

Add experience duration extraction (years, roles, seniority)

Integration with external ATS systems

Deploy via Streamlit Cloud / AWS / Docker

âœï¸ Author: Built with â¤ï¸ by Your Name

âœ… This README now matches your final code â€” with predicted_score integrated everywhere.
